{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModel, BertTokenizer, BertForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_metric, Dataset\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from pytorch_lightning import seed_everything\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_data(path: str) -> tuple[dict, dict]:\n",
    "    train, val = {}, {}\n",
    "    TRESHOLD = 0.4\n",
    "    max_count_train = None\n",
    "    max_count_val = 5000\n",
    "    df = pd.read_csv(path)\n",
    "    min_max_scaler = lambda x: (x - x.min()) / (x.max() - x.min())\n",
    "    df = df[df['views'] > 50000]\n",
    "    df['likes_div_views'] = min_max_scaler(df['likes_div_views'])\n",
    "    df['likes_div_views_trunced'] = (df['likes_div_views'] > TRESHOLD).astype(float)\n",
    "    df['text'] = df['text'].astype(str)\n",
    "    train['text'], val['text'], train['y_prob'], val['y_prob'], train['y_label'], val['y_label'] = \\\n",
    "        train_test_split(df['text'].values, df['likes_div_views'].values, df['likes_div_views_trunced'].values, test_size=0.33, random_state=70)\n",
    "\n",
    "    for key in val.keys():\n",
    "        val[key] = val[key][:max_count_val]\n",
    "    for key in train.keys():\n",
    "        train[key] = train[key][:max_count_train]\n",
    "\n",
    "    print('Train labels:', len(train['text']))\n",
    "    print('Val labels:', len(val['text']))\n",
    "    return train, val\n",
    "\n",
    "\n",
    "train, val = load_data(f'./data/df_all.csv')\n",
    "plt.hist(train['y_prob'], bins=50);\n",
    "plt.show()\n",
    "plt.hist(val['y_prob'], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "seed_everything(42)\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('DeepPavlov/rubert-base-cased-sentence', num_labels=1).to(device)\n",
    "model.config.id2label = {\"0\": \"Haha score\"}\n",
    "model.config.label2id = {\"Haha score\": 0}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased-sentence\")\n",
    "\n",
    "def tokenize_batch(tokenizer, texts:list):\n",
    "    dl = DataLoader(texts, batch_size=8, shuffle=False)\n",
    "    temp = []\n",
    "    for text in dl:\n",
    "        t = tokenizer.batch_encode_plus(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            max_length = 512,\n",
    "            truncation = True,\n",
    "            pad_to_max_length = True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        temp.append(t)\n",
    "    ans = {}\n",
    "    for key in temp[0].keys():\n",
    "        ans[key] = torch.cat([x[key] for x in temp])\n",
    "    return ans\n",
    "\n",
    "\n",
    "tokens_train = tokenize_batch(tokenizer, train['text'])\n",
    "tokens_val = tokenize_batch(tokenizer, val['text'])\n",
    "\n",
    "class DatasetT(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, target):\n",
    "        self.encodings = encodings\n",
    "        self.target = target\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.target[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "train_dataset = DatasetT(tokens_train, train['y_label'])\n",
    "test_dataset = DatasetT(tokens_val, val['y_label'])\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids >= 0.4\n",
    "    preds = pred.predictions[:,[0]]    \n",
    "    prec, recall, thresholds = precision_recall_curve(labels, preds)\n",
    "    f1_scores = 2*recall*prec/(recall+prec)\n",
    "    roc = roc_auc_score(labels, preds)\n",
    "    sigm = lambda x:1/(1 + np.exp(-x))\n",
    "    \n",
    "    return {'ROC AUC': roc, 'maxF1': np.nanmax(f1_scores), 'best_threshold':sigm(thresholds[np.nanargmax(f1_scores)])}\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = f'./UmoreskiAI',\n",
    "    num_train_epochs = 2,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    weight_decay =0.01,\n",
    "    logging_dir = f'.logs',\n",
    "    load_best_model_at_end = True,\n",
    "    learning_rate = 2e-5,\n",
    "    evaluation_strategy ='epoch',\n",
    "    logging_strategy = 'epoch',\n",
    "    save_strategy = 'epoch', \n",
    "    save_total_limit = 1,\n",
    "    seed=21)\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.MSELoss()\n",
    "        loss = loss_fct(F.sigmoid(logits), labels.view(-1,1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = CustomTrainer(model=model,\n",
    "                  tokenizer = tokenizer,\n",
    "                  args = training_args,\n",
    "                  train_dataset = train_dataset,\n",
    "                  eval_dataset = test_dataset,\n",
    "                  compute_metrics = compute_metrics\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
